# âœ¨ Yi Jin Jing: An LLM-driven financial service solution integrating data foundation and intelligent investment


## ğŸ§  Project Background & Design Philosophy

The name â€œYi Jin Jingâ€ is inspired by the ancient Chinese classic *Yi Jin Jing*.

*Yi Jin Jing* emphasizes conditioning the tendons and unblocking meridians to reshape the body's internal operating structure. In this project, it serves as a metaphor: financial texts, market data, knowledge graphs, and intelligent models are viewed as the â€œmeridian systemâ€ of financial information. By deeply integrating LLMs, knowledge bases, and intelligent investment models, we reconstruct the process of collecting, organizing, and transmitting financial knowledge, enabling a smoother, more efficient, and more intelligent flow and transformation of financial information within the system.

Yi Jin Jing is built upon generative AI, knowledge graphs, and graph machine learning technologies to provide an LLM-driven financial service solution that integrates a knowledge base and intelligent investment. Through the FinEX financial knowledge extraction agent, the system performs financial text analysis, financial event extraction, and structured knowledge generation from news, announcements, and public sentiment. Based on a temporal multimodal knowledge graph, it constructs an automated multimodal data foundation. With the self-developed MEHGT multimodal heterogeneous graph neural network prediction model, it generates trading signals and supports intelligent asset allocation. Ultimately, it forms a closed-loop system of â€œknowledge extraction â€” data foundation â€” intelligent decision-making.â€



---

## ğŸ“ Project Structure

This repository adopts a layered directory structure based on functional modules, enabling independent development and joint debugging.

    YiJinJing/
    â”œâ”€â”€ MEHGT model/
    â”‚   â”œâ”€â”€ HGTConv.py
    â”‚   â”œâ”€â”€ han_conv_edge_attr3.py
    â”‚   â”œâ”€â”€ readme.md
    â”‚   â”œâ”€â”€ å›æµ‹demo1.ipynb
    â”‚   â”œâ”€â”€ æµªæ½®ä¿¡æ¯_Grid_é¡ºåº.py
    â”‚   â””â”€â”€ figs/                     # Stores MEHGT-related diagrams & backtest visualizations
    â”‚
    â”œâ”€â”€ FinEX/
    â”‚   â”œâ”€â”€ Merge.sh                  # LoRA weight merging script
    â”‚   â”œâ”€â”€ Quant.sh                  # FinEX quantization & inference script
    â”‚   â”œâ”€â”€ SFT_TF-14B.sh             # SFT (supervised finetuning) main script
    â”‚   â”œâ”€â”€ Web_demo.sh               # Web Demo startup script
    â”‚   â””â”€â”€ readme.md
    â”‚
    â”œâ”€â”€ a series of multimodal knowledge graphs/
    â”‚   â”œâ”€â”€ HeteroG_eventall2.0.ipynb # Event-level hetero graph construction & visualization
    â”‚   â”œâ”€â”€ readme.md
    â”‚   â”œâ”€â”€ toexcel_gpu1.ipynb        # Graph export & feature warehousing
    â”‚   â””â”€â”€ æ¯”äºšè¿ªï¼ˆ21-24ï¼‰_deldata3.xlsx # Industry / stock example data
    â”‚
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ README.md
    â””â”€â”€ requirements.txt              # Python dependency environment

---

## ğŸ”§ Runtime Environment & Dependencies

The project is primarily built on the Python ecosystem. Core dependencies include (but are not limited to):


- Deep learning & LLM frameworksï¼š`PyTorch`ã€`transformers`ã€`peft`ã€`accelerate`
- Graph neural networksï¼š`PyTorch Geometric` and extensions
- Data processing & visualizationï¼š`pandas`ã€`numpy`ã€`matplotlib`ã€`seaborn`
- Web & deploymentï¼š`gradio` or `fastapi`ï¼ˆdepending on integration methodï¼‰

It is recommended to create an isolated environment using `conda` or `venv` and install all dependencies with:

    conda create -n yjj python=3.10
    conda activate yjj
    pip install -r requirements.txt

---

# ğŸ§© Module 1: FinEX (Golden Bean) â€” Financial Knowledge Extraction Agent

## ğŸ“– 1.1 Overview

FinEX (Golden Bean) is a knowledge extraction and structured representation agent designed for the financial domain. It is fine-tuned on the Qwen (Tongyi Qianwen) large model via LoRA-based supervised finetuning. Using high-quality annotated financial corpora and task-specific instructions, it automatically identifies key information (financial events, financial entities, financial actions, etc.) from unstructured texts such as financial news, announcements, and research reports, generating structured tuples (triples/pairs) ready for knowledge storage and graph construction.


## ğŸ§± 1.2 Technical Details & Implementation Roadmap (with diagrams)

-  **Finetuning Framework:** FinEX training follows and extends the LLaMA Factory framework. Training scripts and parameter configs can be found in `FinEX Finetuning/` `.sh` files. Source framework:
  
  - LLaMA Factory: <https://github.com/hiyouga/LLaMA-Factory>

- **Model Base & Configuration:**
  
  - Uses a bilingual financial-enhanced LLM (Tongyi-Finance 14B) as the base.
  - Adopts LoRA/QLoRA for parameter-efficient finetuning, focusing on output and some intermediate layers.
  - Full model weights and config:
    
    - ModelScope: <https://www.modelscope.cn/models/Madness977/FinEX>

- **Task Modeling & Instruction Design:**
  
  - Financial event extraction is formulated as an instruction-driven generation task, with output formats constrained by prompts.
  - Supports multi-level outputs: event triples, indicator pairs, multi-event joint parsing.

- **Key Training Scripts:**

    - `SFT_TF-14B.sh`ï¼šMain SFT workflow (data loading, training strategy, distributed training)
    - `Merge.sh`ï¼šMerge LoRA weights with base model
    - `Quant.sh`ï¼š4bit/8bit quantization & deployment optimization
    - `Web_demo.sh`ï¼šLaunch FinEX web demo

- **Architecture Diagram**

    ![FinEX æ¨¡å‹æ€»ä½“æ¶æ„ç¤ºæ„å›¾](figs/FinEX.png)

- **Web Deployment Example (Bilingual)**  
    ![FinEX ç½‘é¡µéƒ¨ç½²æ¨ç†ç¤ºä¾‹](figs/ä¸­æ–‡çŸ¥è¯†æŠ½å–ç¤ºä¾‹.png)

---

## ğŸ“Š 1.3 Evaluation & Visualization (with diagrams)

- **Tasks:** Financial event extraction & structured tuple generation.
- **Metrics:**
  
  - Text-level: Precision / Recall / F1, sentence-level parsing success rate
  - Structure-level: Triple recall, entity alignment accuracy, event coverage

- **Example Findings:**
  
  - On financial announcements & news datasets, FinEX outperforms baseline LLMs and non-finetuned models across Precision / Recall / F1.
  - Shows strong capability in parsing long texts with multiple events.
- **Example Comparison Results:**

    ![FinEX äº‹ä»¶æŠ½å–ä¸ç»“æ„åŒ–è¡¨ç¤ºæ•ˆæœç¤ºä¾‹](figs/NLPç»“æœ.png)

---

# ğŸŒ Module 2: Temporal Multimodal Knowledge Graph

## ğŸ“– 2.1 Overview

This module uses FinEX output tuples, combined with market data, financial indicators, industry classification, and other information sources, to map discrete financial events, entities, and market signals into a temporal heterogeneous graph. It provides a â€œcomputable, inferable, traceableâ€ data foundation for downstream prediction models.


Key focuses:

- Multimodal fusion (text, numerical, graph structure)
- Temporal sliding updates & version management

## ğŸ§± 2.2 Technical Flow (with diagrams)

-  **Graph Construction Pipeline:**
  
  1. Entity Alignment: Normalize company names, stock tickers, industry labels.
  3. Temporal Slicing: Build daily graph snapshots or sliding-window block graphs.
  4. Graph Storage & Export: Using `HeteroG_eventall2.0.ipynb` & `toexcel_gpu1.ipynb` for construction, visualization, and exporting features.

- **Related Notebooks:**
  
  - `HeteroG_eventall2.0.ipynb`: Construction & visualization of temporal multimodal hetero graphs
  - `toexcel_gpu1.ipynb`ï¼šExport graph features to tables/feature warehouse
  - `BYDï¼ˆ21-24ï¼‰_deldata3.xlsx`ï¼šExample dataset


## ğŸ“Š 2.3 Example


- **neo4j-based Visualization Example:**

    ![æ—¶åºå¤šæ¨¡æ€çŸ¥è¯†å›¾è°±å¯è§†åŒ–ä¸åº”ç”¨ç¤ºä¾‹](figs/neo4j.png)

---

# ğŸ“ˆ Module 3: MEHGT â€” Multimodal Edge-Enhanced Heterogeneous Graph Transformer

## ğŸ“– 3.1 Overview

MEHGT (Multimodal Edge-enhanced Heterogeneous Graph Transformer) is a GNN model featuring:

- A heterogeneous graph transformer architecture for diverse node/edge types
- Edge-level multimodal information (text events, numerical indicators, sentiment signals)
- Temporal modeling via sliding windows & backtesting for trend and risk prediction


## ğŸ§± 3.2 Technical Details (with diagrams)

- **Model Structure:**
  
  - **Input:** Heterogeneous graph sequences (companies, events, industries) with multimodal edge features 
  - **Graph Encoding:** MEHGT / HGTConv / HANConv layers to learn type-specific projections and edge-enhanced attention  
  - **Temporal Modeling:** Transformer / TCN / BiLSTM over multi-day graph embeddings 
  - **Output:** Stock trend classification / risk labels + attention weights for interpretability

## ğŸ“Š 3.3 Prediction & Backtest Evaluation (with diagrams)

- **Prediction Tasks:**
  
  - Short-term stock/industry trend prediction  
  - Detection of abnormal volatility around risk events  

- **Metrics:**
  
  - Classification: Accuracy, Precision, Recall, F1, AUC, MCC 
  - Backtesting: CRR, MDD, Sharpe, win rate, turnover 

- **Backtest Implementation:**
  
  - Logic implemented in `Backtest_demo1.ipynb`, including net value curve, drawdown curve, and comparisons  
  - Comparison with baseline strategies (buy-and-hold, factor models)

- **Example Results:**

    ![MEHGT å›æµ‹ç»“æœä¸æŒ‡æ ‡å¯¹æ¯”ç¤ºæ„å›¾](figs/å¯¹æ¯”ç»“æœ.png)
    ![MEHGT å›æµ‹ç»“æœä¸æŒ‡æ ‡å¯¹æ¯”ç¤ºæ„å›¾](figs/å›æµ‹.png)


---


# âš ï¸ Notes

1. **Data Compliance & Privacy** 
   - Ensure all data sources are legal and compliant, especially announcements or institution data.    
   - Avoid uploading sensitive raw data; apply anonymization and aggregation.

2. **Resource Requirements** 
   - FinEX training requires high-end GPUs (A100/H100); adjust batch size & sequence length accordingly.   
   - MEHGT training is resource-intensive; plan sampling & batching strategies.

3. **Version Compatibility**  
   - PyTorch / CUDA / transformers versions may varyâ€”follow `requirements.txt`.   
   - For multi-node clusters, ensure consistency across NCCL, accelerate, deepspeed, etc.

4. **Reproducibility**
   - Set random seeds and record hyperparameters & data splits.

---

# ğŸ”® Future Outlook

1. **Integration with Financial Service Platforms**  
The solution will be deeply integrated with banking, wealth management, and insurance platforms, providing reusable intelligent financial capabilities. Users will obtain more reliable market insights, risk alerts, and asset allocation suggestions.


2. **Expansion to More Financial Scenarios** 
Leveraging the MEHGT-LKG heterogeneous graph model, the system will expand to derivatives, smart risk control, personalized investment via user profiling, and cross-institution generalization.


3. **Domestic Financial Technology Independence**  
In collaboration with Ascend AI Computing Center (China), the team explores high-performance training & inference based on MindSpore, RAG, CANN, heterogeneous operators, and GNN-related innovations to achieve ecosystem independence and drive local fintech development.

---

# ğŸ“š Appendix: Links & Citation Suggestions

- LLaMA Factoryï¼ˆFinEX finetuning framework referenceï¼‰ï¼š  
  <https://github.com/hiyouga/LLaMA-Factory>

- FinEX model card & weights:
  <https://www.modelscope.cn/models/Madness977/FinEX>


